(nlp_learn) guozhijia@guozhijiadeMacBook-Pro 01-intent-classify % python ./training_code/train_bert.
py
Total samples: 5024, Total labels: 59
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./assets/models/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
{'loss': 4.1193, 'grad_norm': 11.646567344665527, 'learning_rate': 2.7e-06, 'epoch': 0.16}          
{'loss': 3.9262, 'grad_norm': 9.953433990478516, 'learning_rate': 5.7000000000000005e-06, 'epoch': 0.32}
{'loss': 3.5458, 'grad_norm': 9.026374816894531, 'learning_rate': 8.7e-06, 'epoch': 0.48}           
{'loss': 2.8171, 'grad_norm': 8.137819290161133, 'learning_rate': 1.1700000000000001e-05, 'epoch': 0.63}
{'loss': 2.5306, 'grad_norm': 3.749520778656006, 'learning_rate': 1.47e-05, 'epoch': 0.79}          
{'loss': 2.3667, 'grad_norm': 4.821316242218018, 'learning_rate': 1.77e-05, 'epoch': 0.95}          
{'eval_loss': 1.937150239944458, 'eval_accuracy': 0.5741293532338309, 'eval_runtime': 5.4573, 'eval_samples_per_second': 184.158, 'eval_steps_per_second': 2.932, 'epoch': 1.0}                         
{'loss': 2.0402, 'grad_norm': 3.8102784156799316, 'learning_rate': 2.07e-05, 'epoch': 1.11}         
{'loss': 1.7038, 'grad_norm': 3.9604616165161133, 'learning_rate': 2.37e-05, 'epoch': 1.27}         
{'loss': 1.6057, 'grad_norm': 5.524447441101074, 'learning_rate': 2.6700000000000002e-05, 'epoch': 1.43}
{'loss': 1.3265, 'grad_norm': 4.315016269683838, 'learning_rate': 2.97e-05, 'epoch': 1.59}          
{'loss': 1.0413, 'grad_norm': 4.131697177886963, 'learning_rate': 2.696629213483146e-05, 'epoch': 1.75}
{'loss': 1.08, 'grad_norm': 5.379497528076172, 'learning_rate': 2.359550561797753e-05, 'epoch': 1.9}
{'eval_loss': 0.8472452163696289, 'eval_accuracy': 0.8557213930348259, 'eval_runtime': 4.7793, 'eval_samples_per_second': 210.283, 'eval_steps_per_second': 3.348, 'epoch': 2.0}                        
{'loss': 0.9012, 'grad_norm': 4.865898609161377, 'learning_rate': 2.0224719101123596e-05, 'epoch': 2.06}                                                                                                
{'loss': 0.8722, 'grad_norm': 5.673796653747559, 'learning_rate': 1.6853932584269665e-05, 'epoch': 2.22}
{'loss': 0.7871, 'grad_norm': 3.57363224029541, 'learning_rate': 1.348314606741573e-05, 'epoch': 2.38}
{'loss': 0.6994, 'grad_norm': 5.202613830566406, 'learning_rate': 1.0112359550561798e-05, 'epoch': 2.54}
{'loss': 0.616, 'grad_norm': 2.522771120071411, 'learning_rate': 6.741573033707865e-06, 'epoch': 2.7}
{'loss': 0.6907, 'grad_norm': 2.7639880180358887, 'learning_rate': 3.3707865168539327e-06, 'epoch': 2.86}
{'eval_loss': 0.632789671421051, 'eval_accuracy': 0.9233830845771144, 'eval_runtime': 5.3776, 'eval_samples_per_second': 186.886, 'eval_steps_per_second': 2.975, 'epoch': 3.0}                         
{'train_runtime': 437.3738, 'train_samples_per_second': 27.567, 'train_steps_per_second': 0.432, 'train_loss': 1.7589715347088204, 'epoch': 3.0}  