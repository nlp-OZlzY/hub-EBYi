# 使用 SMP_Final_Origin2_10 数据集训练

## 数据处理，如下json形式

```json
[
  {
    "text": "请帮我打开uc",
    "tokens": [
      "请",
      "帮",
      "我",
      "打",
      "开",
      "u",
      "c"
    ],
    "intent": "LAUNCH",
    "domain": "app",
    "slots": [
      "O",
      "O",
      "O",
      "O",
      "O",
      "B-name",
      "I-name"
    ],
    "original_slots": {
      "name": "uc"
    }
  },
  {
    "text": "打开汽车之家",
    "tokens": [
      "打",
      "开",
      "汽",
      "车",
      "之",
      "家"
    ],
    "intent": "LAUNCH",
    "domain": "app",
    "slots": [
      "O",
      "O",
      "B-name",
      "I-name",
      "I-name",
      "I-name"
    ],
    "original_slots": {
      "name": "汽车之家"
    }
  },
  {
    "text": "帮我打开人人",
    "tokens": [
      "帮",
      "我",
      "打",
      "开",
      "人",
      "人"
    ],
    "intent": "LAUNCH",
    "domain": "app",
    "slots": [
      "O",
      "O",
      "O",
      "O",
      "B-name",
      "I-name"
    ],
    "original_slots": {
      "name": "人人"
    }
  }
]
```

## 训练代码

```python
import sys
import os
import json
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import BertTokenizer, get_linear_schedule_with_warmup
from torch.optim import AdamW
from sklearn.model_selection import train_test_split
from tqdm import tqdm
import numpy as np

# Add project root to sys.path
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(CURRENT_DIR)
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

from model.bert_joint import BertJointModel, JointDataset
from config import BERT_MODEL_PERTRAINED_PATH, BERT_JOINT_MODEL_PATH, SMP_DATA_META_PATH

def compute_metrics(intent_preds, intent_labels, slot_preds, slot_labels, id2slot):
    # Intent Accuracy
    intent_acc = (np.array(intent_preds) == np.array(intent_labels)).mean()
    
    # Slot F1 (Simplified token-level F1, excluding padding)
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    
    for pred_seq, label_seq in zip(slot_preds, slot_labels):
        for p, l in zip(pred_seq, label_seq):
            if l == -100: continue # Skip padding/special tokens
            
            p_tag = id2slot.get(p, "O")
            l_tag = id2slot.get(l, "O")
            
            if p_tag == l_tag and p_tag != "O":
                true_positives += 1
            elif p_tag != "O" and p_tag != l_tag:
                false_positives += 1
            elif l_tag != "O" and p_tag == "O":
                false_negatives += 1
                
    precision = true_positives / (true_positives + false_positives + 1e-9)
    recall = true_positives / (true_positives + false_negatives + 1e-9)
    f1 = 2 * precision * recall / (precision + recall + 1e-9)
    
    # Slot Accuracy
    total_tokens = 0
    correct_tokens = 0
    for pred_seq, label_seq in zip(slot_preds, slot_labels):
        for p, l in zip(pred_seq, label_seq):
            if l == -100: continue
            total_tokens += 1
            if p == l:
                correct_tokens += 1
    slot_acc = correct_tokens / (total_tokens + 1e-9)

    return intent_acc, f1, slot_acc
def train():
    # Parameters (Aligned with training_code/train_bert.py)
    BATCH_SIZE = 64      # 调整为16，与单任务一致，适合显存较小的环境
    EPOCHS = 8          # 增加轮数，因为样本少了，需要更多轮次过拟合
    LR = 3e-5            # 恢复到 3e-5 (标准BERT微调)，禁用 CRF 后不再需要极低学习率
    # WARMUP_STEPS will be calculated dynamically
    WEIGHT_DECAY = 0.01  # 增加权重衰减，防止过拟合
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # 快速验证模式：限制样本数量
    USE_FAST_DEBUG = False # 关闭快速验证，使用全量数据
    DEBUG_SAMPLE_SIZE = 200 # (未使用)
    
    print(f"Using device: {DEVICE}")
    if USE_FAST_DEBUG:
        print(f"!!! FAST DEBUG MODE ENABLED: Using only {DEBUG_SAMPLE_SIZE} samples !!!")
    
    # Resolve paths
    meta_path = os.path.join(PROJECT_ROOT, SMP_DATA_META_PATH)
    train_file = os.path.join(PROJECT_ROOT, 'assets/dataset/SMP_processed/train.json')
    dev_file = os.path.join(PROJECT_ROOT, 'assets/dataset/SMP_processed/dev.json')
    # Note: BERT_MODEL_PERTRAINED_PATH is likely relative too, let's fix it for tokenizer
    model_path = os.path.join(PROJECT_ROOT, BERT_MODEL_PERTRAINED_PATH)
    output_model_path = os.path.join(PROJECT_ROOT, BERT_JOINT_MODEL_PATH)

    # Load Meta
    with open(meta_path, 'r', encoding='utf-8') as f:
        meta = json.load(f)
        
    intent_map = meta['intent_map']
    slot_map = meta['slot_map']
    id2slot = {v: k for k, v in slot_map.items()}
    
    # Load Data
    # 发现官方 dev.json 分布与 train.json 差异巨大，导致验证指标虚低。
    # 现改为从 train.json 中切分 10% 作为验证集，保证同分布。
    with open(train_file, 'r', encoding='utf-8') as f:
        full_data = json.load(f)
    
    # Stratified split might be better but random split is fine for 5000 samples
    train_data, dev_data = train_test_split(full_data, test_size=0.1, random_state=42)
    print(f"Train samples: {len(train_data)}, Validation samples: {len(dev_data)}")

    # Calculate Slot Weights (Class Balancing)
    all_slot_labels = []
    for item in train_data:
        slots = item['slots']
        for s in slots:
            all_slot_labels.append(slot_map.get(s, 0))
    
    # Also include O labels from padding logic (implicitly O is dominant)
    # Just counting raw labels gives a good approximation
    label_counts = np.bincount(all_slot_labels, minlength=len(slot_map))
    total_counts = len(all_slot_labels)
    
    # Inverse Frequency Weights
    # weight = total / (num_classes * count)
    # Add smoothing to avoid division by zero
    weights = total_counts / (len(slot_map) * (label_counts + 1))
    
    # Normalize weights so they sum to num_classes (optional, but good for stability)
    # Or just ensure O weight is low.
    
    # Manually reduce weight of "O" if it's too dominant
    o_id = slot_map.get("O", 0)
    print(f"Original Weight for O (ID {o_id}): {weights[o_id]:.4f}")
    
    # Convert to Tensor
    slot_weights = torch.tensor(weights, dtype=torch.float).to(DEVICE)
    print(f"Slot weights calculated. Max: {weights.max():.4f}, Min: {weights.min():.4f}")

    # Tokenizer
    tokenizer = BertTokenizer.from_pretrained(model_path, max_length=64)
    
    # Datasets
    if USE_FAST_DEBUG:
        train_data = train_data[:DEBUG_SAMPLE_SIZE]
        dev_data = dev_data[:DEBUG_SAMPLE_SIZE]
        
    train_dataset = JointDataset(train_data, tokenizer, intent_map, slot_map)
    dev_dataset = JointDataset(dev_data, tokenizer, intent_map, slot_map)
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)
    
    # Model
    # Initialize with CRF=True to create the layer, but we will dynamically control it
    model = BertJointModel.from_pretrained(
        model_path,
        num_intents=len(intent_map),
        num_slots=len(slot_map),
        use_crf=True, 
        slot_weights=slot_weights
    )
    model.to(DEVICE)
    
    # Curriculum Learning Strategy
    CRF_START_EPOCH = int(EPOCHS * 0.75) # 兜底策略：最晚在训练的后 25% 阶段开启 CRF
    CRF_F1_THRESHOLD = 0.5   # 激进策略：只要 F1 超过 0.3（打破全 O 僵局），立即开启
    print(f"Curriculum Learning: CRF enabled if Epoch >= {CRF_START_EPOCH} OR Val_F1 > {CRF_F1_THRESHOLD}")
    
    # Optimizer (With weight decay fix)
    # 不对 bias 和 LayerNorm 进行权重衰减，这是 BERT 训练的标准操作
    no_decay = ['bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {
            'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
            'weight_decay': WEIGHT_DECAY
        },
        {
            'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
            'weight_decay': 0.0
        }
    ]
    
    optimizer = AdamW(optimizer_grouped_parameters, lr=LR)
    
    # Dynamic Warmup
    total_steps = len(train_loader) * EPOCHS
    warmup_steps = int(total_steps * 0.1)
    print(f"Total training steps: {total_steps}, Warmup steps: {warmup_steps}")
    
    scheduler = get_linear_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=warmup_steps, 
        num_training_steps=total_steps
    )
    
    best_val_f1 = 0
    
    for epoch in range(EPOCHS):
        # Determine CRF usage for this epoch (Hybrid Strategy)
        # 1. Fallback: Force enable after N epochs
        # 2. Adaptive: Enable immediately if model shows signs of learning (F1 > threshold)
        use_crf_now = (epoch >= CRF_START_EPOCH) or (best_val_f1 > CRF_F1_THRESHOLD)
        
        print(f"\nEpoch {epoch+1}/{EPOCHS} | CRF Enabled: {use_crf_now} (Best F1: {best_val_f1:.4f})")
        
        # Training
        model.train()
        train_loss = 0
        
        train_intent_preds = []
        train_intent_labels = []
        train_slot_preds = []
        train_slot_labels = []

        for batch in tqdm(train_loader, desc="Training"):
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            intent_labels = batch['intent_labels'].to(DEVICE)
            slot_labels = batch['slot_labels'].to(DEVICE)
            
            # DEBUG: Print first sample of first batch to verify Tokenizer fix
            if train_loss == 0 and epoch == 0: 
                print("\n--- DEBUG INFO ---")
                print(f"Sample Input IDs: {input_ids[0].cpu().tolist()}")
                print(f"Decoded: {tokenizer.decode(input_ids[0].cpu())}")
                print(f"Intent Label: {intent_labels[0].item()}")
                print("------------------\n")

            model.zero_grad()
            outputs = model(
                input_ids, 
                attention_mask=attention_mask,
                intent_labels=intent_labels,
                slot_labels=slot_labels,
                use_crf_override=use_crf_now
            )
            
            # outputs: (total_loss, intent_logits, slot_preds)
            # wait, model forward returns loss if labels provided.
            # Let's inspect model output again. 
            # In BertJointModel.forward: returns (total_loss, intent_logits, slot_preds)
            
            loss = outputs[0]
            
            # Debug Loss Components (Optional, if we returned them. Currently model sums them up)
            # To debug, we rely on total loss.
            
            # Debug NaN
            if torch.isnan(loss):
                print("Warning: Loss is NaN, skipping batch")
                continue

            loss.backward()
            
            # Gradient Clipping (防止梯度爆炸导致NaN)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            scheduler.step()
            
            train_loss += loss.item()
            
            # Record predictions for training metrics
            with torch.no_grad():
                intent_logits = outputs[1]
                # Intent Preds
                preds = torch.argmax(intent_logits, dim=1).cpu().numpy()
                labels = intent_labels.cpu().numpy()
                train_intent_preds.extend(preds)
                train_intent_labels.extend(labels)
                
                # Slot Preds
                # Note: outputs[2] contains slot_preds. 
                # If CRF is used, it's a list of lists. If not (Softmax), it might be a list of lists (converted in model forward).
                # Checking model code: In both cases (CRF or Softmax), model.forward returns slot_preds as list of lists.
                current_slot_preds = outputs[2]
                current_slot_labels = slot_labels.cpu().numpy()
                
                # Align lengths (same logic as validation)
                for i, pred_seq in enumerate(current_slot_preds):
                    label_seq = current_slot_labels[i]
                    if len(pred_seq) < len(label_seq):
                        pred_seq = pred_seq + [0] * (len(label_seq) - len(pred_seq))
                    elif len(pred_seq) > len(label_seq):
                        pred_seq = pred_seq[:len(label_seq)]
                    train_slot_preds.append(pred_seq)
                    train_slot_labels.append(label_seq)

        avg_train_loss = train_loss / len(train_loader)
        train_intent_acc, train_slot_f1, train_slot_acc = compute_metrics(
            train_intent_preds, train_intent_labels,
            train_slot_preds, train_slot_labels,
            id2slot
        )
        print(f"Average Training Loss: {avg_train_loss:.4f}")
        print(f"Training Intent Accuracy: {train_intent_acc:.4f}")
        print(f"Training Slot Accuracy: {train_slot_acc:.4f} (F1: {train_slot_f1:.4f})")
        
        # Validation
        model.eval()
        intent_preds, intent_labels_list = [], []
        slot_preds, slot_labels_list = [], []
        
        for batch in tqdm(dev_loader, desc="Validation"):
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            intent_labels = batch['intent_labels'].to(DEVICE)
            slot_labels = batch['slot_labels'].to(DEVICE)
            
            with torch.no_grad():
                # outputs: (loss, intent_logits, slot_preds_list)
                # Validation should use same CRF strategy as current training phase?
                # Usually validation should reflect best inference capability.
                # Let's align it with current training phase to see progress.
                outputs = model(input_ids, attention_mask=attention_mask, use_crf_override=use_crf_now)
                intent_logits = outputs[1]
                current_slot_preds = outputs[2] # List[List[int]] from CRF
                
            # Intent Predictions
            intent_preds.extend(torch.argmax(intent_logits, dim=1).cpu().numpy())
            intent_labels_list.extend(intent_labels.cpu().numpy())
            
            # Slot Predictions
            # current_slot_preds is already list of lists of tag indices
            # Need to handle padding for metric computation compatibility
            # slot_labels is [batch, seq_len] tensor
            
            current_slot_labels = slot_labels.cpu().numpy()
            
            for i, pred_seq in enumerate(current_slot_preds):
                label_seq = current_slot_labels[i]
                
                # Truncate or pad prediction to match label length if necessary
                # CRF decode usually matches input sequence length (excluding padding if masked)
                # But here we padded input manually.
                
                # Align lengths for metric function
                # Metric function ignores -100 in labels, so we just need ensuring zip works
                
                # If pred_seq is shorter (due to masking), we can pad with O (or ignore)
                # If longer, truncate.
                
                if len(pred_seq) < len(label_seq):
                    pred_seq = pred_seq + [0] * (len(label_seq) - len(pred_seq))
                elif len(pred_seq) > len(label_seq):
                    pred_seq = pred_seq[:len(label_seq)]
                    
                slot_preds.append(pred_seq)
                slot_labels_list.append(label_seq)
            
        # Metrics
        intent_acc, slot_f1, slot_acc = compute_metrics(
            intent_preds, intent_labels_list, 
            slot_preds, slot_labels_list, 
            id2slot
        )
        
        print(f"Validation Intent Accuracy: {intent_acc:.4f}")
        print(f"Validation Slot Accuracy: {slot_acc:.4f} (F1: {slot_f1:.4f})")
        
        # Save Best Model (Force save for testing)
        if True:
            best_val_f1 = slot_f1
            torch.save(model.state_dict(), output_model_path)
            print(f"Model saved to {output_model_path}")

if __name__ == "__main__":
    train()
```