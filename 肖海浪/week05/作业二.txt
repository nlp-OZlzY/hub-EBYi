# BERT文本编码与相似度计算技术方案

​	方案采用“BERT向量检索”完成FAQ相似问匹配。数据侧：每条FAQ包含标准问法（标题）、若干相似问法（可维护多条，上限可达200）、答案、生效时间、类目与标签等元数据。

 	离线阶段：对标准问法与所有相似问法做文本清洗（去空格、统一大小写/全半角、去敏感符号），输入中文BERT获取句向量（可取[CLS]向量或Mean Pooling），将向量与FAQ_ID写入向量库（FAISS/Milvus/ES向量），并同步元数据到关系型库用于过滤。

​	在线阶段：用户提问→同样清洗→BERT编码得到向量→向量库TopK召回→计算余弦相似度并重排→按类目/标签/生效时间过滤后取Top1；若Top1相似度≥阈值则返回该FAQ答案，否则进入兜底（推荐相关问题、转人工、或接入大模型RAG基于知识库生成答复）。最后通过命中率、Top1准确率、人工接管率持续迭代阈值与相似问覆盖。

## 一、核心技术选型

### 模型选择

- **基础模型**：bert-base-chinese（Google官方中文预训练模型）。
- **隐藏层维度**：768维。
- **优势**：双向Transformer架构，捕捉上下文语义。



### 相似度计算方法

- **余弦相似度**：cosine_similarity = (A·B) / (||A|| × ||B||)。
- **优势**：不依赖向量长度，仅关注方向相似性。



## 二、BERT文本编码流程

### 1. 文本预处理

```python
# 分词与Tokenization
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')

# 用户提问预处理
def preprocess_text(text):
    # 去除特殊字符，标准化标点
    text = text.strip()
    # 截断或补充至512 tokens
    tokens = tokenizer.tokenize(text)[:510]
    tokens = ['[CLS]'] + tokens + ['[SEP]']
    return tokens
```



### 2. 编码转换

```python
# 将tokens转换为模型输入格式
def encode_text(tokens):
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    attention_mask = [1] * len(input_ids)  # 标记有效token
    # padding处理
    padding_length = 512 - len(input_ids)
    input_ids.extend([0] * padding_length)
    attention_mask.extend([0] * padding_length)
    return {
        'input_ids': torch.tensor([input_ids]),
        'attention_mask': torch.tensor([attention_mask])
    }
```



### 3. BERT编码

```python
from transformers import BertModel

model = BertModel.from_pretrained('bert-base-chinese')

def get_bert_embedding(text):
    # 1. 文本预处理
    tokens = preprocess_text(text)
    
    # 2. 编码转换
    inputs = encode_text(tokens)
    
    # 3. BERT前向传播
    with torch.no_grad():
        outputs = model(**inputs)
    
    # 4. 提取句向量（使用[CLS] Token）
    cls_embedding = outputs.last_hidden_state[:, 0, :]
    
    return cls_embedding.numpy()  # 返回768维向量
```



## 三、相似度计算方案



### 1. 离线FAQ向量化

```python
# 批量处理FAQ库，生成向量索引
def build_faq_index(faq_list):
    faq_embeddings = []
    for faq in faq_list:
        # 对标准问题进行编码
        embedding = get_bert_embedding(faq['question'])
        faq_embeddings.append({
            'faq_id': faq['id'],
            'embedding': embedding,
            'answer': faq['answer']
        })
    return faq_embeddings
```



### 2. 在线相似度匹配

```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

def match_user_query(user_query, faq_index, threshold=0.75):
    # 1. 编码用户问题
    query_embedding = get_bert_embedding(user_query)
    
    # 2. 计算与所有FAQ的相似度
    similarities = []
    for faq in faq_index:
        sim = cosine_similarity(
            query_embedding, 
            faq['embedding'].reshape(1, -1)
        )[0][0]
        similarities.append({
            'faq_id': faq['faq_id'],
            'similarity': sim,
            'answer': faq['answer']
        })
    
    # 3. 排序并筛选
    similarities.sort(key=lambda x: x['similarity'], reverse=True)
    
    # 4. 返回超过阈值的结果
    results = [item for item in similarities if item['similarity'] >= threshold]
    
    return results
```



## 四、技术优化策略



### 1. 性能优化

- **向量索引加速 **：使用FAISS进行快速近似搜索。
- **缓存机制 **：Redis缓存高频问题的embedding和匹配结果。
- **批量处理 **：FAQ更新时批量重新计算embedding。



### 2. 准确率提升

- **多候选匹配 **：返回Top-3结果，由客服确认。
- **相似提问融合 **：将标准问题与相似问题一起编码。
- **阈值动态调整 **：根据类目、时间段等维度设置不同阈值。



### 3. 部署方案

- **模型服务化 **：使用FastAPI部署BERT推理服务。
- **异步处理 **：FAQ更新异步触发embedding重算。
- **监控告警 **：监控相似度分布，异常时自动降级到关键词匹配。



## 五、实现步骤总结

1. **环境搭建 **：安装transformers、torch、sklearn
2. **模型加载 **：下载bert-base-chinese模型
3. **FAQ向量化 **：离线生成所有FAQ的768维向量
4. **接口开发 **：实现query→embedding→similarity的API
5. **性能测试 **：测试响应时间（目标<200ms）
6. **准确率评估**：人工标注测试集，评估Top-1/Top-3准确率