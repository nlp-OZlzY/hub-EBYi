import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
# BertForSequenceClassification bert ç”¨äº æ–‡æœ¬åˆ†ç±»
# Trainerï¼š ç›´æ¥å®ç° æ­£å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—ã€å‚æ•°æ›´æ–°
# TrainingArgumentsï¼š è¶…å‚æ•°ã€å®éªŒè®¾ç½®

from sklearn.preprocessing import LabelEncoder
from datasets import Dataset
import numpy as np

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
dataset_df = pd.read_csv("../online_shopping_10_cats.csv")
# ğŸ‘‡ å…³é”®ï¼šå…ˆæ‰“ä¹±æ•´ä¸ªæ•°æ®é›†ï¼
dataset_df = dataset_df.sample(frac=1, random_state=42).reset_index(drop=True)
# åˆå§‹åŒ– LabelEncoderï¼Œç”¨äºå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾
lbl = LabelEncoder()

# å–å‰ N æ¡
N = 2000
subset_df = dataset_df.iloc[:N]
# æå–å¹¶ç¡®ä¿æ˜¯å­—ç¬¦ä¸²
texts = subset_df['review'].astype(str).tolist()
labels = lbl.fit_transform(subset_df['cat'])
# # æ‹Ÿåˆæ•°æ®å¹¶è½¬æ¢å‰500ä¸ªæ ‡ç­¾ï¼Œå¾—åˆ°æ•°å­—æ ‡ç­¾
# labels = lbl.fit_transform(dataset_df['cat'].values[:N])
# # æå–å‰500ä¸ªæ–‡æœ¬å†…å®¹
# texts = list(dataset_df['review'].values[:N])

# æ£€æŸ¥ç±»åˆ«æ•°
print(f"ä½¿ç”¨çš„ç±»åˆ«æ•°: {len(np.unique(labels))}")  # åº”è¯¥æ˜¯ 10
print(f"å„ç±»åˆ«æ ·æœ¬æ•°:\n{pd.Series(labels).value_counts().sort_index()}")

# åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
x_train, x_test, train_labels, test_labels = train_test_split(
    texts,             # æ–‡æœ¬æ•°æ®
    labels,            # å¯¹åº”çš„æ•°å­—æ ‡ç­¾
    test_size=0.2,     # æµ‹è¯•é›†æ¯”ä¾‹ä¸º20%
    stratify=labels    # ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ ‡ç­¾åˆ†å¸ƒä¸€è‡´
)




# ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('../models/google-bert/bert-base-chinese')
model = BertForSequenceClassification.from_pretrained('../models/google-bert/bert-base-chinese', num_labels=10)

# ä½¿ç”¨åˆ†è¯å™¨å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–‡æœ¬è¿›è¡Œç¼–ç 
# truncation=Trueï¼šå¦‚æœæ–‡æœ¬è¿‡é•¿åˆ™æˆªæ–­
# padding=Trueï¼šå¯¹é½æ‰€æœ‰åºåˆ—é•¿åº¦ï¼Œå¡«å……åˆ°æœ€é•¿
# max_length=64ï¼šæœ€å¤§åºåˆ—é•¿åº¦
train_encodings = tokenizer(x_train, truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(x_test, truncation=True, padding=True, max_length=128)

# å°†ç¼–ç åçš„æ•°æ®å’Œæ ‡ç­¾è½¬æ¢ä¸º Hugging Face `datasets` åº“çš„ Dataset å¯¹è±¡
train_dataset = Dataset.from_dict({
    'input_ids': train_encodings['input_ids'],           # æ–‡æœ¬çš„token ID
    'attention_mask': train_encodings['attention_mask'], # æ³¨æ„åŠ›æ©ç 
    'labels': train_labels                               # å¯¹åº”çš„æ ‡ç­¾
})
test_dataset = Dataset.from_dict({
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': test_labels
})





# å®šä¹‰ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
def compute_metrics(eval_pred):
    # eval_pred æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ¨¡å‹é¢„æµ‹çš„ logits å’ŒçœŸå®çš„æ ‡ç­¾
    logits, labels = eval_pred
    # æ‰¾åˆ° logits ä¸­æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå³é¢„æµ‹çš„ç±»åˆ«
    predictions = np.argmax(logits, axis=-1)
    # è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡å¹¶è¿”å›ä¸€ä¸ªå­—å…¸
    return {'accuracy': (predictions == labels).mean()}

# é…ç½®è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',              # è®­ç»ƒè¾“å‡ºç›®å½•ï¼Œç”¨äºä¿å­˜æ¨¡å‹å’ŒçŠ¶æ€
    num_train_epochs=4,                  # è®­ç»ƒçš„æ€»è½®æ•°
    per_device_train_batch_size=16,      # è®­ç»ƒæ—¶æ¯ä¸ªè®¾å¤‡ï¼ˆGPU/CPUï¼‰çš„æ‰¹æ¬¡å¤§å°
    per_device_eval_batch_size=16,       # è¯„ä¼°æ—¶æ¯ä¸ªè®¾å¤‡çš„æ‰¹æ¬¡å¤§å°
    warmup_steps=500,                    # å­¦ä¹ ç‡é¢„çƒ­çš„æ­¥æ•°ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒï¼Œ step å®šä¹‰ä¸º ä¸€æ¬¡ æ­£å‘ä¼ æ’­ + å‚æ•°æ›´æ–°
    weight_decay=0.01,                   # æƒé‡è¡°å‡ï¼Œç”¨äºé˜²æ­¢è¿‡æ‹Ÿåˆ
    logging_dir='./logs',                # æ—¥å¿—å­˜å‚¨ç›®å½•
    logging_steps=100,                   # æ¯éš”100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    eval_strategy="epoch",               # æ¯è®­ç»ƒå®Œä¸€ä¸ª epoch è¿›è¡Œä¸€æ¬¡è¯„ä¼°
    save_strategy="epoch",               # æ¯è®­ç»ƒå®Œä¸€ä¸ª epoch ä¿å­˜ä¸€æ¬¡æ¨¡å‹
    load_best_model_at_end=True,         # è®­ç»ƒç»“æŸååŠ è½½æ•ˆæœæœ€å¥½çš„æ¨¡å‹
    metric_for_best_model="accuracy",
)

# å®ä¾‹åŒ– Trainer ç®€åŒ–æ¨¡å‹è®­ç»ƒä»£ç 
trainer = Trainer(
    model=model,                         # è¦è®­ç»ƒçš„æ¨¡å‹
    args=training_args,                  # è®­ç»ƒå‚æ•°
    train_dataset=train_dataset,         # è®­ç»ƒæ•°æ®é›†
    eval_dataset=test_dataset,           # è¯„ä¼°æ•°æ®é›†
    compute_metrics=compute_metrics,     # ç”¨äºè®¡ç®—è¯„ä¼°æŒ‡æ ‡çš„å‡½æ•°
)

# æ·±åº¦å­¦ä¹ è®­ç»ƒè¿‡ç¨‹ï¼Œæ•°æ®è·å–ï¼Œepoch batch å¾ªç¯ï¼Œæ¢¯åº¦è®¡ç®— + å‚æ•°æ›´æ–°

# å¼€å§‹è®­ç»ƒæ¨¡å‹
trainer.train()
# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
trainer.evaluate()

# trainer æ˜¯æ¯”è¾ƒç®€å•ï¼Œé€‚åˆè®­ç»ƒè¿‡ç¨‹æ¯”è¾ƒè§„èŒƒåŒ–çš„æ¨¡å‹
# å¦‚æœæˆ‘è¦å®šåˆ¶åŒ–è®­ç»ƒè¿‡ç¨‹ï¼Œtraineræ— æ³•æ»¡è¶³

# è®­ç»ƒå®Œæˆåï¼Œæ˜¾å¼ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° output_dir æ ¹ç›®å½•
trainer.save_model("./results")  # ğŸ‘ˆ å…³é”®ï¼è¿™ä¼šç”Ÿæˆ pytorch_model.bin

# ====================================================
# æ–°å¢ï¼šç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æ–°æ ·æœ¬ï¼ˆæ¨ç†ï¼‰
# ====================================================

# 1. ä¿å­˜ LabelEncoderï¼ˆä»¥ä¾¿åç»­åŠ è½½ä½¿ç”¨ï¼‰
import joblib
joblib.dump(lbl, './results/label_encoder.pkl')

# 2. åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆTrainer å·²è‡ªåŠ¨ä¿å­˜åœ¨ output_dirï¼‰
from transformers import BertTokenizer, BertForSequenceClassification
import torch
import numpy as np

# é‡æ–°åŠ è½½ tokenizerï¼ˆå’Œè®­ç»ƒæ—¶ä¸€è‡´ï¼‰
tokenizer = BertTokenizer.from_pretrained('../models/google-bert/bert-base-chinese')
# åŠ è½½å¾®è°ƒåçš„æ¨¡å‹ï¼ˆè‡ªåŠ¨åŠ è½½ best modelï¼‰
model = BertForSequenceClassification.from_pretrained('./results')
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# 3. åŠ è½½æ ‡ç­¾ç¼–ç å™¨
lbl = joblib.load('./results/label_encoder.pkl')

# 4. å®šä¹‰é¢„æµ‹å‡½æ•°
def predict(text: str, max_length=128):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        padding=True,
        max_length=max_length
    )
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits
        pred_id = torch.argmax(logits, dim=-1).item()
        confidence = torch.softmax(logits, dim=-1).max().item()
    pred_label = lbl.inverse_transform([pred_id])[0]
    return pred_label, confidence

# 5. æµ‹è¯•æ–°æ ·æœ¬
print("\nğŸ” å¼€å§‹æµ‹è¯•æ–°æ ·æœ¬ï¼š")
test_samples = [
    "è¿™æ¬¾æ‰‹æœºæ‹ç…§ç‰¹åˆ«æ¸…æ™°ï¼Œç”µæ± ä¹Ÿå¾ˆè€ç”¨ï¼",
    "è‹¹æœå¾ˆæ–°é²œï¼Œå°±æ˜¯æœ‰ç‚¹è´µã€‚",
    "ä¹¦çš„å†…å®¹å¾ˆæœ‰æ·±åº¦ï¼Œå€¼å¾—åå¤é˜…è¯»ã€‚",
    "çƒ­æ°´å™¨å®‰è£…åä¸€ç›´æ¼æ°´ï¼Œå®¢æœä¹Ÿä¸ç®¡ã€‚",
    "è¿™ä»¶è¡£æœå°ºç åå°ï¼Œè´¨é‡ä¸€èˆ¬ã€‚"
]

for sample in test_samples:
    pred, conf = predict(sample)
    print(f"è¾“å…¥: {sample}")
    print(f"é¢„æµ‹ç±»åˆ«: {pred} (ç½®ä¿¡åº¦: {conf:.2f})\n")