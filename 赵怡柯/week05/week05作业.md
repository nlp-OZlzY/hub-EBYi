# 作业1

> （400字文档）: 阅读上面的客服工作台的说明，总结我们自己作为后端开发和算法开发的角色，我们需要做什么？  
>
> - 需要设计数据库吗？   
>
> - 需要使用什么模型？    
>
> - 如何使用使用bert的？    
>
> - 是否需要使用大模型？ 

**需要设计数据库，**

1. 用来存储数据，比如：支撑类目、FAQ、相似问法等。
2. 实现数据的增删改查、批量导入导出（适配 3000 条导入上限）

**模型部分主要使用BERT中文预训练模型。**

1. 进行语义编码，提取语义向量：将用户提问、FAQ 标准问法及相似问法输入 BERT 模型做统一的语义编码，提取文本的高维语义向量。
2. 计算相似度，实现精准匹配：通过向量相似度计算实现精准匹配。

**无需使用大模型。**

1. 因核心需求是固定 FAQ 的相似度匹配，非生成式问答，BERT 的语义表征能力已能满足需求
2. 且大模型部署成本高、推理效率低，不符合客服工作台的实时匹配要求。
3. 同时算法层需实现匹配阈值调优、相似问法的批量管理，保障匹配命中率。

# 作业2

> 作业2（400字文档， 流程图）: 
>
> 如何使用bert 进行文本编码，并且使用bert 进行相似度计算，需要写清楚技术方案；

本方案以 BERT-base 中文预训练模型为核心，完成用户提问与 FAQ 文本的语义编码及相似度计算，实现精准的问句匹配，核心分文本预处理、BERT 编码、向量相似度计算三步。

1. **文本预处理**：对用户提问、FAQ 标题及相似问法做统一清洗，去除特殊符号、冗余空格，进行中文分词与 BERT 专属的文本格式化（添加 [CLS] 句首标识、[SEP] 句尾标识，做固定长度的 padding/truncation，生成 token_id、segment_id、attention_mask）。
2. **BERT 文本编码**：将预处理后的文本张量输入预训练 BERT 模型，冻结模型底层参数，取 [CLS] 位置的输出向量作为整段文本的**语义特征向量**，将用户提问向量、FAQ 文本向量统一映射至同一高维语义空间。
3. 相似度计算
   - 余弦相似度

​	计算用户提问向量与各 FAQ 文本向量的相似度值，设置合理的匹配阈值（如 0.8），筛选出阈值以上的结果，取相似度最高的 FAQ 对应的回答返回给用户；若无达标结果，可返回关联问题或无匹配结果提示。



本方案中 BERT 模型可基于中文语料微调，优化短文本（客服 FAQ）的语义表征能力，提升匹配精准度。

流程图如下：
<img width="2160" height="1440" alt="image" src="https://github.com/user-attachments/assets/6f90de45-4bb6-4019-93e1-0ce93a558931" />
