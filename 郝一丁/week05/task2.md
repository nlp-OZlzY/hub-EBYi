# 用 BERT 做文本编码与相似度计算

整体思路是把用户问句和FAQ 问句都转换成同一种可比较的语义向量，然后用向量相似度找到最接近的一条，直接返回它对应的答案。

## 1.文本编码  
我们使用中文 BERT 作为编码器。输入一条问句后，先做基础的分词与截断,保证长度不超过模型上限，再送入 BERT 得到句子的语义表示。为了让表示更稳定，通常会对句子里各个词的表示做一个聚合,例如取平均或类似方式，得到一个固定长度的句向量。让语义相近的问句在向量空间里也更接近。


## 2.建立 FAQ 向量库 
对每条 FAQ 的标准问法以及维护的相似问法，提前批量编码成向量，并与 FAQ 的编号、类目、答案等信息绑定存储。这样线上查询时不需要重复计算历史问句的向量，能显著提升响应速度。

## 3.相似度检索**
用户提问到来时，同样用 BERT 编码成向量，然后与向量库中的 FAQ 向量计算相似度（常用余弦相似度或点积）。系统取相似度最高的若干条作为候选，并根据业务规则选出最终命中项，返回对应答案。

## 4.精排
如果计算资源充足并希望更准，可以在先召回一小批候选后，再用更强但更慢的模型逐对比较用户问句与候选问句，重新排序后输出最优结果。

